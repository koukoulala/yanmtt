# 12.9
export CUDA_VISIBLE_DEVICES=0,1
python pretrain_mbart.py -n 1  -nr 0 -g 2  --model_path examples/models/try_mbart_model --use_official_pretrained --pretrained_model=../ckpt/mbart-large-cc25 --tokenizer_name_or_path ../ckpt/mbart-large-cc25 --langs hi,en,vi --mono_src examples/data/train.hi,examples/data/train.en,examples/data/train.vi --shard_files
nohup python -u pretrain_mbart.py -n 1  -nr 0 -g 2  --model_path examples/models/try_mbart_model --use_official_pretrained --pretrained_model=../ckpt/mbart-large-cc25 --tokenizer_name_or_path ../ckpt/mbart-large-cc25 --langs en,fr --mono_src ../datasets/small_NTG/xglue.ntg.en.src.train,../datasets/small_NTG/xglue.ntg.fr.src.train --shard_files --lr=1e-5 &> logs/try_pretrain.out &
"python -m pip install virtualenv --user && python -m virtualenv /tmp/env_pretrain_en && . /tmp/env_pretrain_en/bin/activate && python -m pip install -r requirements.txt && cd transformers && python setup.py install && cd .. && python -u pretrain_mbart.py -n 1  -nr 0 -g 2  --model_path=res/pretrain_en/12.9 --use_official_pretrained --pretrained_model=[#input-previous-model-path] --tokenizer_name_or_path=[#input-previous-model-path] --langs=en --mono_src=[#input-training-data-path]/xglue.ntg.en.src.train --shard_files --lr=1e-5 --batch_size=1024 --no_reload_optimizer_ctr_and_scheduler"

nohup python -u generate_mbart.py -n 1  -nr 0 -g 1 --model_path examples/models/try_generate --use_official_pretrained --pretrained_model=../ckpt/mbart-large-cc25 --tokenizer_name_or_path=../ckpt/mbart-large-cc25 --dev_slang en --dev_tlang en --dev_src ../datasets/small_NTG/xglue.ntg.en.src.train --dev_tgt ../datasets/small_NTG/xglue.ntg.en.src.train --shard_files  &> logs/try_generate.out &

# 12.15
"python -m pip install virtualenv --user && python -m virtualenv /tmp/env_generate_en && . /tmp/env_generate_en/bin/activate && python -m pip install -r requirements.txt && cd transformers && python setup.py install && cd .. && python -u generate_mbart.py -n 1  -nr 0 -g 1  --model_path=res/generate_en/12.14 --use_official_pretrained --pretrained_model=[#input-previous-model-path]/en --tokenizer_name_or_path=[#input-previous-model-path]/en --dev_slang=en --dev_tlang=en --dev_src=[#input-training-data-path]/xglue.ntg.en.src.train --dev_tgt=[#input-training-data-path]/xglue.ntg.en.src.train --shard_files --no_reload_optimizer_ctr_and_scheduler"

"python -m pip install virtualenv --user && python -m virtualenv /tmp/env_pretrain_en && . /tmp/env_pretrain_en/bin/activate && python -m pip install -r requirements.txt && cd transformers && python setup.py install && cd .. && python -u pretrain_mbart.py -n 1  -nr 0 -g 1  --model_path=res/pretrain_en/12.14 --use_official_pretrained --pretrained_model=[#input-previous-model-path] --tokenizer_name_or_path=[#input-previous-model-path] --langs=en --mono_src=[#input-training-data-path]/xglue.ntg.en.src.train --shard_files --lr=1e-5 --batch_size=1024 --no_reload_optimizer_ctr_and_scheduler"

# 12.16
"python -m pip install virtualenv --user && python -m virtualenv /tmp/env_generate_en && . /tmp/env_generate_en/bin/activate && python -m pip install -r requirements.txt && cd transformers && python setup.py install && cd .. && python -u generate_mbart.py -n 1  -nr 0 -g 1  --model_path=res/generate_en/12.16 --use_official_pretrained --pretrained_model=[#input-previous-model-path]/en --tokenizer_name_or_path=[#input-previous-model-path]/en --dev_slang=en --dev_tlang=en --dev_src=[#input-training-data-path]/xglue.ntg.en.src.train --dev_tgt=[#input-training-data-path]/xglue.ntg.en.src.train --shard_files --no_reload_optimizer_ctr_and_scheduler"

# 12.19
"python -m pip install virtualenv --user && python -m virtualenv /tmp/env_pretrain_de && . /tmp/env_pretrain_de/bin/activate && python -m pip install -r requirements.txt && cd transformers && python setup.py install && cd .. && python -u pretrain_mbart.py -n 1  -nr 0 -g 1  --model_path=res/pretrain_de/12.19 --use_official_pretrained --pretrained_model=[#input-previous-model-path] --tokenizer_name_or_path=[#input-previous-model-path] --langs=en,de --mono_src=[#input-training-data-path]/xglue.ntg.en.src.train,[#input-training-data-path]/xglue.ntg.de.src.train --shard_files --lr=1e-5 --batch_size=1024 --no_reload_optimizer_ctr_and_scheduler"
"python -m pip install virtualenv --user && python -m virtualenv /tmp/env_pretrain_@@lang@@ && . /tmp/env_pretrain_@@lang@@/bin/activate && python -m pip install -r requirements.txt && cd transformers && python setup.py install && cd .. && python -u pretrain_mbart.py -n 1  -nr 0 -g 1  --model_path=res/pretrain_@@lang@@/12.19 --use_official_pretrained --pretrained_model=[#input-previous-model-path] --tokenizer_name_or_path=[#input-previous-model-path] --langs=en,@@lang@@ --mono_src=[#input-training-data-path]/xglue.ntg.en.src.train,[#input-training-data-path]/xglue.ntg.@@lang@@.src.train --shard_files --lr=1e-5 --batch_size=1024 --no_reload_optimizer_ctr_and_scheduler"


# 4.8
python create_autoconfig.py ./tokenizers/mbart-25 mbart ./data/sample.all .
python create_autoconfig.py ./tokenizers/mbart-vienhi16k mbart ./data/sample.all .

# 4.13
python create_autoconfig_mBART.py data/sampled_data/combined_data.EN,data/sampled_data/combined_data.DE,data/sampled_data/combined_data.ZH 16000 tokenizers/mbart-test mbart "<2all>" 1000000 1
python create_autoconfig_mBART.py ../MultilingualAdsGeneration/data/pre_train_data/combined_data 64000 tokenizers/mbart-ads mbart "<2all>" 1000000 1

# 4.14
export CUDA_VISIBLE_DEVICES=0,1
nohup python -u pretrain_mBART.py -n 1  -nr 0 -g 2  --model_path res/try_mbart_model --tokenizer_name_or_path ./tokenizers/mbart-test --mono_src ./data/sampled_data --encoder_layers 2 --decoder_layers 2 --encoder_attention_heads=4 --decoder_attention_heads=4 --encoder_ffn_dim=258 --decoder_ffn_dim=258 --d_model=128 --shard_files --no_lang_identifier=1 &> logs/try_pretrain_mBART.out &
nohup python -u pretrain_nmt.py -n 1  -nr 0 -g 2 --model_path res/try_mbart_model --tokenizer_name_or_path ./tokenizers/mbart-test --langs FR,EN --mono_src ./data/sampled_data/combined_data.FR,./data/sampled_data/combined_data.EN --encoder_layers 1 --decoder_layers 1 --encoder_attention_heads=1 --decoder_attention_heads=1 --encoder_ffn_dim=128 --decoder_ffn_dim=128 --d_model=64 --shard_files --no_lang_identifier=1 &> logs/try_pretrain_mBART.out &
nohup python -u pretrain_mBART.py -n=1  -nr=0 -g=2  --model_path=res/try_mbart_model --tokenizer_name_or_path=./tokenizers/mbart-test --langs=EN-DE-FR-NL-ES-IT-DA-SV-NO-ZH-FI-PT-other --mono_src=./data/sampled_data --encoder_layers=1 --decoder_layers=1 --encoder_attention_heads=1 --decoder_attention_heads=1 --encoder_ffn_dim=128 --decoder_ffn_dim=128 --d_model=64 --no_lang_identifier=1 &> logs/try_pretrain_mBART_2.out &

"python -m pip install virtualenv --user && python -m virtualenv /tmp/env_pretrain && . /tmp/env_pretrain/bin/activate && python -m pip install -r simple_requirements.txt && cd transformers && python setup.py install && cd .. && python -u pretrain_mBART.py -n=1  -nr=0 -g=4  --model_path=res/pretrain_mBART --tokenizer_name_or_path=[#input-previous-model-path] --langs=EN-DE-FR-NL-ES-IT-DA-SV-NO-ZH-FI-PT-other --mono_src=[#input-training-data-path] --shard_files --lr=1e-5 --batch_size=1024 --encoder_layers=2 --decoder_layers=2 --encoder_attention_heads=4 --decoder_attention_heads=4 --encoder_ffn_dim=2048 --decoder_ffn_dim=2048 --d_model=512 --no_lang_identifier=1"
"python -m pip install virtualenv --user && python -m virtualenv /tmp/env_pretrain_2 && . /tmp/env_pretrain_2/bin/activate && python -m pip install -r simple_requirements.txt && python -u pretrain_mBART.py -n=1  -nr=0 -g=@@gpu@@  --model_path=res/pretrain_mBART --tokenizer_name_or_path=[#input-previous-model-path] --langs=EN-DE-FR-NL-ES-IT-DA-SV-NO-ZH-FI-PT-other --mono_src=[#input-training-data-path] --shard_files --lr=1e-5 --batch_size=1024 --encoder_layers=2 --decoder_layers=2 --encoder_attention_heads=4 --decoder_attention_heads=4 --encoder_ffn_dim=2048 --decoder_ffn_dim=2048 --d_model=512 --no_lang_identifier=1"